گرادیان نزولی تصادفی (Stochastic Gradient Descent - SGD)

SGD یکی از روش‌های بهینه‌سازی در یادگیری ماشین است که برای کمینه کردن تابع هزینه استفاده می‌شود. این روش یک نسخه سریع‌تر اما نویزی‌تر از گرادیان نزولی دسته‌ای (Batch Gradient Descent) است.

1. تفاوت SGD با گرادیان نزولی کلاسیک

در گرادیان نزولی کلاسیک (Batch Gradient Descent)، گرادیان تابع هزینه روی کل مجموعه داده‌ها محاسبه می‌شود، اما در SGD گرادیان فقط با استفاده از یک نمونه تصادفی از داده‌ها در هر گام محاسبه می‌شود.

مقایسه روش‌ها:
2. فرمول به‌روزرسانی در SGD

فرض کنیم تابع هزینه را بنامیم و همان وزن‌های مدل باشد. فرمول به‌روزرسانی در SGD به این صورت است:

\theta := \theta - \alpha \cdot \frac{\partial J}{\partial \theta} 

اما در SGD به‌جای استفاده از کل داده‌ها، فقط یک نمونه تصادفی را انتخاب می‌کنیم و گرادیان را نسبت به آن محاسبه می‌کنیم:

\theta := \theta - \alpha \cdot \frac{\partial J(\theta, x_i, y_i)}{\partial \theta} 

تفاوت کلیدی با روش کلاسیک

به‌جای استفاده از میانگین تمام داده‌ها، فقط یک نمونه از داده‌ها در هر مرحله برای محاسبه گرادیان استفاده می‌شود.

این باعث می‌شود که به‌روزرسانی‌ها سریع‌تر ولی پرنوسان‌تر باشند.

3. مزایا و معایب SGD

مزایا:

:white_check_mark: سرعت بالاتر نسبت به روش دسته‌ای، مخصوصاً در مجموعه داده‌های بزرگ.
:white_check_mark: می‌تواند از مینیمم‌های محلی فرار کند و به مینیمم سراسری نزدیک‌تر شود.
:white_check_mark: برای مسائل یادگیری عمیق و شبکه‌های عصبی بسیار مناسب است.

معایب:

:x: به دلیل استفاده از یک نمونه تصادفی در هر گام، نوسان زیادی دارد.
:x: ممکن است در مینیمم‌های محلی نوسان کند و همگرا نشود.
:x: نیاز به انتخاب مناسب نرخ یادگیری (Learning Rate) دارد.

4. روش‌های بهبود SGD

چندین تکنیک وجود دارند که می‌توانند عملکرد SGD را بهبود دهند:

:white_check_mark: ۴.۱. SGD + Momentum

هدف: کاهش نوسانات و افزایش سرعت همگرایی.

ایده: به هر به‌روزرسانی، مقداری از به‌روزرسانی قبلی را اضافه می‌کنیم تا مدل بهتر به سمت مینیمم حرکت کند.

فرمول:

v := \beta v + (1 - \beta) \frac{\partial J}{\partial \theta} \theta := \theta - \alpha v 

که در آن مقدار مومنتوم را تعیین می‌کند (معمولاً ۰.۹).

:white_check_mark: ۴.۲. SGD + RMSprop

هدف: تنظیم خودکار نرخ یادگیری برای پارامترهای مختلف.

ایده: مقدار گرادیان‌های قبلی را ذخیره کرده و نرخ یادگیری را برای هر پارامتر به‌طور جداگانه تنظیم می‌کند.

:white_check_mark: ۴.۳. Adam (Adaptive Moment Estimation)

ترکیبی از Momentum و RMSprop است.

رایج‌ترین روش برای یادگیری عمیق و بهینه‌سازی شبکه‌های عصبی.

فرمول پیچیده‌تر ولی بسیار پایدار و سریع است.
5. جمع‌بندی

SGD یک روش سریع و پرنوسان برای بهینه‌سازی مدل‌های یادگیری ماشین است.

به‌جای استفاده از کل مجموعه داده، فقط یک نمونه تصادفی برای محاسبه گرادیان استفاده می‌کند.

مزیت اصلی آن سرعت بالا است، ولی می‌تواند نوسان داشته باشد.

روش‌هایی مانند Momentum، RMSprop و Adam به بهبود همگرایی آن کمک می‌کنند.
